# -*- coding: utf-8 -*-
"""HackerEarth_InfyEdgeVerve_NLP_Classification_00.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lff5kUbywy55W9gTRl3mdzUQiLEJtPk2

Hacker Earth Infy Edge Verve NLP Classification

Importing the Libraries
"""

import keras
import nltk
import pandas as pd
import numpy as np
import re
import codecs
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score,mean_absolute_error

"""Importing Files"""

train_dataset = pd.read_csv("Train.csv")
test_dataset = pd.read_csv("Test.csv")
train_dataset = pd.concat([train_dataset,test_dataset],axis =0, sort=False,ignore_index=True)

"""Data Preprocessing"""

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, len(train_dataset)):
    review = re.sub('[^a-zA-Z]', ' ', str(train_dataset['Item_Description'][i]))
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

"""Creating the Bag of Words model"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 4096)
X4 = cv.fit_transform(corpus).toarray()

X1 = train_dataset.iloc[:,1]
X1  = X1.values.reshape((len(X1),1))
onehotencoder_X1 = OneHotEncoder()
onehotencoder_X1 = onehotencoder_X1.fit(X1)
X1 = onehotencoder_X1.transform(X1).toarray()

X2 = train_dataset.iloc[:,2]
X2  = X2.values.reshape((len(X2),1))
onehotencoder_X2 = OneHotEncoder()
onehotencoder_X2 = onehotencoder_X2.fit(X2)
X2 = onehotencoder_X2.transform(X2).toarray()

X3 = train_dataset.iloc[:,3]
X3  = X3.values.reshape((len(X2),1))

y = train_dataset.iloc[:5566,-1]
y  = y.values.reshape((len(y),1))
labelencoder_y = LabelEncoder()
labelencoder_y = labelencoder_y.fit(y)
y = labelencoder_y.transform(y)

y_onehot = train_dataset.iloc[:5566,-1]
y_onehot  = y_onehot.values.reshape((len(y_onehot),1))
onehotencoder_y = OneHotEncoder()
onehotencoder_y = onehotencoder_y.fit(y_onehot)
y_onehot = onehotencoder_y.transform(y_onehot).toarray()

y_onehot.shape

X_train_test = np.concatenate((X1,X2,X3,X4),axis=1)
X = X_train_test[:5566,:]
X.shape

"""Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35)

"""Prediction Using All Classifier

KNN Classifier
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=5,metric='minkowski', p=2)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Support Vector Machine"""

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 250, criterion = 'entropy')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Neural Network"""

from sklearn.neural_network import MLPClassifier
classifier = MLPClassifier(hidden_layer_sizes=(1600,800,400,128,64 ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='invscaling', learning_rate_init=0.0001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score    
Accuracy=accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(Accuracy*100,'%')

"""Deep Learning"""

X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size = 0.35)

from keras import layers
from keras.layers import Dense
from keras.models import Sequential
from keras.regularizers import l2
classifier = Sequential()
classifier.add(Dense(units = 2048, activation = 'relu',input_shape=(3285,)))
classifier.add(layers.Dropout(0.2))
classifier.add(Dense(units = 1024, activation = 'relu'))
classifier.add(layers.Dropout(0.2))
classifier.add(Dense(units = 512, activation = 'relu'))
#classifier.add(layers.Dropout(0.3))
classifier.add(Dense(units = 256, activation = 'relu'))
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 64, activation = 'relu'))
classifier.add(Dense(units = 36, activation = 'softmax'))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = classifier.fit(x=X_train, y=y_train, batch_size=64, epochs=100, verbose=1, callbacks=None, validation_split=0.0, validation_data=(X_test,y_test), shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)
y_pred = classifier.predict(X_test)

"""Training full data"""

DESIRED_ACCURACY = 1.0

class myCallback(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>=DESIRED_ACCURACY):
      print("\nReached "+str(DESIRED_ACCURACY)+"% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

X_submit = X_train_test[5566:,:]
from keras import layers
from keras.layers import Dense
from keras.models import Sequential
from keras.regularizers import l2
classifier = Sequential()
classifier.add(Dense(units = 2048, activation = 'relu',input_shape=(3285,)))
classifier.add(layers.Dropout(0.2))
classifier.add(Dense(units = 1024, activation = 'relu'))
classifier.add(layers.Dropout(0.2))
classifier.add(Dense(units = 512, activation = 'relu'))
#classifier.add(layers.Dropout(0.3))
classifier.add(Dense(units = 256, activation = 'relu'))
classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 64, activation = 'relu'))
classifier.add(Dense(units = 36, activation = 'softmax'))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = classifier.fit(x=X, y=y_onehot, batch_size=64, epochs=100, verbose=1, callbacks=[callbacks], validation_split=0.0, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)
y_pred = classifier.predict(X_test)
y_submit = classifier.predict(X_submit)

y_submit1=np.where(y_submit==1)[1]

y_submit1

y_pred = labelencoder_y.inverse_transform(y_submit1)

y_pred

np.savetxt("submission.csv", y_pred, delimiter=",",fmt='%s')